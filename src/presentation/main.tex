\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[preprint]{common/neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[]{placeins}


\setlength{\parindent}{0pt}

\title{Rotten Tomatoes Review Discrepancy}
\author{%
  Benjamin Raible\\
  Matrikelnummer 6413200\\
  \texttt{b.raible@student.uni-tuebingen.de} \\
  \And
  Daniel Kerezsy\\
  Matrikelnummer 6300575\\
  \texttt{daniel.kerezsy@student.uni-tuebingen.de} \\
  \And
  Andreas Kotzur\\
  Matrikelnummer 6479284\\
  \texttt{andreas.kotzur@student.uni-tuebingen.de} \\
}

\begin{document}
\maketitle
\begin{abstract}
    This project investigates the discrepancies between official critics' and the audience's ratings for movies on Rotten Tomatoes. We want to find out if there are any regularities and if those are enough to predict the scores or their discrepancy/divergence. This project covers the data gathering and the evaluation with null-hypothesis tests, linear regression, decision trees and MLPs. We conclude that there is not enough information to infer any significant results.
\end{abstract}

\section{Introduction}
    There are a lot of websites rating the quality/perception of movies and TV series. One of the most popular ones is the US-American website \href{https://www.rottentomatoes.com}{Rotten Tomatoes} \cite{rotten_about} which allows officially registered critics and the general audience to rate a movie resulting in a score between 0\% and 100\% for both voter categories. It is a well-known phenomenon that the opinions of these two parties often diverge, and sometimes even by a huge amount. This observation has led to many discussions \cite{looper_divides}\cite{reddit_divide_1}\cite{reddit_divide_2}.\\
    We want to know if there are any commonalities between the movies where ratings diverge and thus need a database where all this information is combined.  
    We gather information about movies from the 1950s up to 2022 using Wikipedia lists and Rotten Tomatoes. We did not use \href{https://www.imdb.com/}{IMDb}, because relevant data like \textit{budget} or \textit{box office revenue} is hidden behind a paywall (IMDb Pro).

\section{Creating the Database}
    Since we could not find a compact and accessible database containing the data we need, the first step was to create our own database. We gathered the data via webscraping. For this process we used the library \textit{scrapy}. Furthermore we cleaned up the resulting data.\\

\subsection{Collecting the data}
% Benno plz correct me on this one
    We started by initially crawling through Wikipedia collecting URLs of entries for the movies together with their corresponding year of release. These movies are listed under sublists, containing all movies released worldwide in that year. A problem we faced was, that the websites on Wikipedia are not uniformly structured. This made crawling more difficult.\\
    In a second crawler we used these URLs to gather information such as the name of the movie and further information. \\
    These Wikipedia entries do not contain a link to their corresponding entry on Rotten Tomatoes. To acquire this entry without needing several steps (search, compare, filter) to eventually get to the corresponding entry, we used a heuristic approach:\\
    We found out that urls of the movies on Rotten Tomatoes have the following structure. They start with a fixed prefix \texttt{"www.rottentomatoes.com/m/"} and end with the title of the movie. The spaces in the title are replaced by underscores.\\
    This is modified for remakes or different movies with the same name, e.g. Batman (1943), Batman (1966) and Batman (1989)\cite{batman_remakes}.
    Again we used a heuristic to acquire the correct entry by simply appending the year to the query as in \texttt{"$<$movie\_name$>$\_$<$yyyy$>$"}. If we didn't get a result in this way, we used the first heuristic.\\
    We than combined the data we got from Rotten Tomatoes with the data we got from Wikipedia. This way we acquired 19366 entries. %\\
    % The process of the data collection is also visualized in this Activity Diagram(\ref{fig:data_col_flow}).
    % % Might have to be deleted since it takes up too much space
    % \begin{figure}[tb]
    %     \centering
    %     \includegraphics[width=1\textwidth]{imgs/activity_diagram.png}
    %     \caption{Data Collection Activity Diagram}
    %     \label{fig:data_col_flow}
    % \end{figure}

\subsection{Cleaning the data}
    In order to verify the combination of data from Rotten Tomatoes and Wikipedia, we extracted the release date gathered from Wikipedia and compared it with the date gathered from Rotten Tomatoes. We removed all entries where the two dates differed. In this process we discarded 222 entries. We assumed that movies with the same name released in the same year are identical. \\
    One of our main goals for the data extraction was to gather information about box office revenue and budget. Since our list contains movies from all around the world starting in the 1950s, we had a lot of different currencies subjected to different inflation rates within our database. Furthermore, some of the Wikipedia entries already contain the converted currencies adjusted for inflation. For standardisation, we convert all entries to US dollar with the inflation being accounted up to December 2022. We considered 15 different currencies, the release date and the potentially already converted values plus their corresponding date.\\
    To calculate the inflation rate we used the average Consumer Price Index (CPI) for all items obtained from a database by the \href{https://data.imf.org/?sk=388dfa60-1d26-4ade-b505-a05a558d9a42}{International Monetary Fond (IMF)}. Using a \href{https://www.kaggle.com/datasets/thebasss/currency-exchange-rates}{dataset containing 52 different currencies} which is also based on the IMF data, we converted all currencies into US dollar.

    % Needed for next section
    % \begin{figure}[h]
    %     \centering
    %     \includegraphics[width=1\textwidth]{imgs/var_p.png}
    %     \caption{FIXME}
    %     \label{fig:variance_p}
    % \end{figure}

% \clearpage
\section{Processing the data}
    % Nur gruppen mit >=100 occurences f√ºr genres und suppliers.
    % Critics Mean: 61.916190970420345, Deviation: 27.822854062556722
    % Audience Mean: 60.35768595041322, Deviation: 21.49258710416916
    % Divergence Mean: 0.9719551988711527, Deviation: 21.060031368055885
    
    % \begin{table}[h]
    %     \centering
    %     \begin{tabular}{c||c|c}
    %          & Mean & Std. Deviation \\ \hline
    %         Critics Score & 61.92 & 27.82 \\ \hline
    %         Audience Score & 60.36 & 21.49 \\ \hline
    %         Divergence & 0.97 & 21.06 
    %     \end{tabular}
    %     \caption{Mean and Standard Deviation of our targets}
    %     \label{tab:mean_std_target}
    % \end{table}

\subsection{Hypothesis tests}
    First we decided to perform hypothesis tests to see, if our parameters influence our targets. Audience and critics score on Rotten Tomatoes measure the percentage of people who submitted a positive review. It makes sense to measure the distance between these scores. This extends to their discrepancy. Therefore our target scores are metric quantities. This allows us to find the correct hypothesis tests.
    \paragraph{Parameters with multiple groups}
    For parameters that divide the data into multiple groups the corresponding hypothesis test is a simple analysis of variance combined with a post hoc test. In our case such parameters are the genre, streaming supplier and the release month.\\
    For our simple analysis of variance the two hypotheses are as follows:
    \begin{enumerate}
        \item[H0:] The mean scores of all groups are equal.
        \item[H1:] At least one of the mean scores of the groups differs.
    \end{enumerate}
    We set our significance level to 0.05.\\
    \noindent
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{imgs/var_p.png}
        \caption{p-values of a simple anova}
        \label{fig:variance_p}
    \end{figure}\\
    % \FloatBarrier % Bitte auskommentiert lassen
    In figure \ref{fig:variance_p} we see that for all three parameters we reject the null hypothesis for each of these three scores.\\
    To further analyze between which groups there is a significant difference in the mean, we use post-hoc tests. The post hoc test uses a student's t-test to test differences between each pair of groups. The hypotheses for the student's t-test are as follows:
    \begin{enumerate}
        \item[H0:] The mean scores of the two groups are equal.
        \item[H1:] The mean scores of the two groups are different.
    \end{enumerate}
    %For a significance level of 0.05 we get that the following percentages of the groups reject the null hypothesis (\ref{fig:hypo_rejection}).
    In figure \ref{fig:hypo_rejection} we see the percentages of the groups that reject the null hypothesis for a significance level of 0.05.
    \noindent
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{imgs/prozent.png}
        \caption{percentage of the pairs of groups with significant deviation in mean assessed by a student's t-test}
        \label{fig:hypo_rejection}
    \end{figure}
    % \FloatBarrier
    From this we can assume that genres, suppliers and release month offer use useful information for predicting our three target scores.
    \paragraph{Metric parameters}
    For metric parameters correlation is usually a good score to look at when deciding whether a parameter contains useful information for our prediction or not. We used the pearsonr test, that has zero correlation as its null hypothesis and a non-zero correlation as its alternative hypothesis which returns p-values as depicted in figure \ref{fig:metric}.\\
    \noindent
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.94\textwidth]{imgs/metric_p.png}
        \caption{p-values of the pearsonr test}
        \label{fig:metric}
    \end{figure}\\
    For year and film-length we reject the null hypothesis for all three target scores. For the other parameter we only have enough evidence to reject the null hypothesis for some of them. Since all of the parameters have a significant correlation to at least some of the target scores, we will include them in our training.
    
    
\subsection{Regression}
    As stated in the abstract, our goal was to predict the critics' score, the audience's score or the difference between the former (i.e. divergence).
    We used several different methods to transform our input parameters with significance to one of the three possible targets. All of the tested methods resulted in a bad fit.
    \paragraph{LinearRegression}
    We tried fitting the data with a simple linear regression model from the scikit-learn library. This generally resulted in the model predicting the mean of the targets for the training samples, whereas the Audience Model was the only one which could apply this to the test samples as well. Both Divergence and especially the Critics Model performed conclusively worse (\ref{tab:lin_regression}).
    \begin{table}[h]
        \centering
        \begin{tabular}{c||c|c|c|c}
             & Training score & Test score & Training MAE & Test MAE \\
            \hline
            Critics Model       & 0.057 & -945.89   & 22.83     & 39.68 \\
            \hline
            Audience Model      & 0.08  & 0.077     & 17.15     & 17.18 \\
            \hline
            Divergence Model    & 0.01  & -6.88     & 16.39     & 17.25
        \end{tabular}
        \caption{Linear Regression Scores and Mean Absolute Errors (MAE)}
        \label{tab:lin_regression}
    \end{table}
    \vspace{-8pt}
    \paragraph{CatBoost}
    CatBoost works on the theory of decision trees. It uses many weak models sequentially, to create a strong model. To achieve our target we used the Regressor (\texttt{CatBoostRegressor}) model from the CatBoost library.
    We saw that the model suffers from overfitting (\ref{fig:mae_errors}). We tried to mitigate this by introducing L2-Regularizers, randomness in the choices on the decision-tree of CatBoost and adjusting the learning rate. Neither helped. 
    
    \paragraph{Neural Networks with PyTorch}
    Using PyTorch we created a model with three Hidden Layers (hidden width = 2 times input width) using Tanh, Leaky ReLU and ReLU as activation functions. It was only possible to use a small batch size since otherwise our model would then only generalize to predict the mean (similar to linear regression) thus the results depicted in figure \ref{fig:mae_errors} are created using a batch size of one. But similarly to the CatBoost Regressor, only with slightly higher mean errors and less iterations, we achieve overfitting of the training samples and slight reduction in the test error. 
    
    \section{Conclusion}
    We conclude that our parameters probably do not contain enough information to predict neither the scores nor the divergence. Our relatively low quantity of data (8000 training samples) is most likely too sparse for our objective and thus the algorithm only overfits without any significant generalization. But we could also observe that the predictions for the audience's score were more accurate, indicating that the audience score is possibly most influenced by the parameters we chose. There are more parameters such as production company, gender ratio of cast and producers that could be extracted from the data we already gathered. Many more can be found by scraping more sources of data. The number of samples is much harder to increase, because the number of films released with scores on Rotten Tomatoes is limited.
    \begin{figure}[tb]
        \centering
        \includegraphics[width=1\textwidth]{imgs/regression_errors.png}
        \caption{Mean Absolute Errors during training of Neural Network- and CatBoost Regressors}
        \label{fig:mae_errors}
    \end{figure}

\clearpage
% \appendix
\bibliographystyle{abbrv}
\bibliography{sources}

\end{document}